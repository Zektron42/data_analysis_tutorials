{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Dark Art of Coding:\n",
    "## Introduction to Python\n",
    "Web scraping\n",
    "\n",
    "<img src='../images/dark_art_logo.600px.png' width='300' style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, students should expect to:\n",
    "\n",
    "* socket module\n",
    "* urllib module\n",
    "* beautiful soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCP\n",
    "\n",
    "* Sends data across a secure(ish) pipe\n",
    "* Once data is recieved it double checks by sending the same data back to make sure it's the right data\n",
    "* Once the check is complete then it finishes and returns the fully checked data to the application that asked for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Port numbers\n",
    "\n",
    "* Application specific points for communicating across the internet\n",
    "* Multiple ports allow multiple applications to talk across the internet without interfering\n",
    "* Typically certain TCP connections have default port numbers\n",
    "\n",
    "Task | Port\n",
    ":----|:----\n",
    "Telnet | 23\n",
    "SSH | 22\n",
    "HTTP | 80\n",
    "HTTPS | 443\n",
    "SMTP (E-mail) | 25\n",
    "DNS (Domain Name) | 53\n",
    "FTP (File Transfer) | 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP (Hyper Text Transfer Protocol)\n",
    "\n",
    "* The standard Protocol for most applications on the internet\n",
    "* Invented to retrieve HTML, images, Documents, etc.\n",
    "* Basic concept:\n",
    "    * Make a connection\n",
    "    * Request a document\n",
    "    * Retrieve the document\n",
    "    * Close the connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://  | www.py4e.com | /lessons/network\n",
    ":--------|:-------------|:----------------\n",
    "Protocol | Host         | Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# HTTP\n",
    "\n",
    "* Connect to `www.discordapp.com`\n",
    "* Request document using this data packet `GET www.discordapp.com/nitro HTTP 1.0`\n",
    "* Get sent html document\n",
    "* Browser renders html\n",
    "* Closes connection when done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTTP requests in python using urllib\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "# First we have to import the requests module from urllib\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "file = urllib.request.urlopen('http://data.pr4e.org/romeo.txt') # urllib let's us open web pages like files\n",
    "\n",
    "# We cycle through every line and print out the data one line at a time\n",
    "\n",
    "for line in file:\n",
    "    print(line.decode().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'what': 1, 'with': 1, 'moon': 1, 'through': 1, 'envious': 1, 'and': 3, 'soft': 1, 'pale': 1, 'east': 1, 'already': 1, 'breaks': 1, 'sick': 1, 'window': 1, 'sun': 2, 'fair': 1, 'Juliet': 1, 'Who': 1, 'But': 1, 'yonder': 1, 'the': 3, 'Arise': 1, 'light': 1, 'It': 1, 'is': 3, 'kill': 1, 'grief': 1}\n"
     ]
    }
   ],
   "source": [
    "# Just like with other files before we can do similar things with txt files downloaded off the web\n",
    "# Like counting the words\n",
    "\n",
    "file = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "\n",
    "count = {}\n",
    "\n",
    "for line in file:\n",
    "    \n",
    "    # We take the line and turn it into a string isntead of a bytes object\n",
    "    #     Then we strip the newline\n",
    "    #     Then we split it on spaces\n",
    "    words = line.decode().strip().split()\n",
    "    \n",
    "    # We cycle through the words one at a time\n",
    "    for word in words:\n",
    "        \n",
    "        # If a key for the word already exists .get() grabs the value otherwise it automatically returns 0\n",
    "        count[word] = count.get(word, 0) + 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicode and python text\n",
    "\n",
    "* All python strings (in py3.+) are internally unicode\n",
    "* When we talk to a network we usually have to encode and decode our data (usually to utf-8)\n",
    "* When we recieve data we recieve it usually as a bytes object which we then pass through a `.decode()` method to get a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'> b'Who is already sick and pale with grief\\n'\n"
     ]
    }
   ],
   "source": [
    "# The most important line of code to any new programmer\n",
    "\n",
    "print(type(line), line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Who is already sick and pale with grief\\n'\n",
      "Who is already sick and pale with grief\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The difference between outputting a bytes object vs. a string\n",
    "\n",
    "print(line)\n",
    "print(line.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading web pages\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>The First Page</h1>\n",
      "<p>\n",
      "If you like, you can switch to the\n",
      "<a href=\"http://www.dr-chuck.com/page2.htm\">\n",
      "Second Page</a>.\n",
      "</p>\n"
     ]
    }
   ],
   "source": [
    "# We pull in the page using urllib.request.urlopen()\n",
    "\n",
    "page = urllib.request.urlopen('http://dr-chuck.com/page1.htm')\n",
    "\n",
    "for line in page:\n",
    "    print(line.decode().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful soup\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Makes reading and parsing web pages a lot easier\n",
    "* Allows you to grab tags of only certain types\n",
    "* You can check certain tags relationship in the heirarchy\n",
    "* Getting links became a whole lot easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the command line\n",
    "\n",
    "*`conda install beautifulsoup4`*\n",
    "\n",
    "## In a python file/interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the necessay modules\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "# Grab the html text\n",
    "\n",
    "htmlText = urllib.request.urlopen('http://dr-chuck.com/page1.htm').read()\n",
    "\n",
    "# Use bs4 to create a soup object from our html text and a \n",
    "#     string argument to let bs4 know what it's looking at\n",
    "\n",
    "soup = BeautifulSoup(htmlText, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To get a list of tags of a certain type you would do this\n",
    "\n",
    "tags = soup('a')  # 'a' tags are called anchor tags and they are used for links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.dr-chuck.com/page2.htm\n"
     ]
    }
   ],
   "source": [
    "# Let's cycle through the tags and get the 'href' data portion. this is the data that contains the link itself\n",
    "\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping\n",
    "---\n",
    "\n",
    "## What is web scraping\n",
    "\n",
    "* Going to a site pretending to be a browser\n",
    "* Looking at the data from that site\n",
    "* And extracting the information you need from it\n",
    "* Typically this is done over and over again on multiple sites\n",
    "\n",
    "## Why web scrape?\n",
    "\n",
    "* Get data from a site that can't export it's data\n",
    "* Collect information on sites to build a search engine database\n",
    "* Monitor sites for changes\n",
    "* Collect social data (who is connected to who?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
